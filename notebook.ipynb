{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import catppuccin\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "\n",
    "mpl.style.use(catppuccin.PALETTE.mocha.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import kagglehub\n",
    "    dataset_path = kagglehub.dataset_download(\"minasameh55/king-country-houses-aa\")\n",
    "    dataset_path = Path(dataset_path)\n",
    "except ModuleNotFoundError:\n",
    "    dataset_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469531e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_path:\n",
    "    dataset_path = Path(\".\")\n",
    "df = pd.read_csv(dataset_path / \"king_ country_ houses_aa.csv\")\n",
    "df.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6484239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d02796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1fe1d",
   "metadata": {},
   "source": [
    "target variable is price\n",
    "date can be probably dropped (post dates, just one year thus temporal dynamics isn't available)\n",
    "no NaNs\n",
    "half of yr_renovated values are 0 which is probably \"not known\"\n",
    "categorical columns are already encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94fc955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.eval(\"is_renovated=yr_renovated>2005\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc66d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"date\", inplace=True)\n",
    "categorical_columns = [\"waterfront\", \"view\", \"condition\", \"grade\", \"zipcode\", \"is_renovated\"]\n",
    "numerical_columns = [col for col in df.columns if col not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"correlation_matrix.png\", dpi=150)\n",
    "plt.close()\n",
    "# several features show strong correlation and might be excluded\n",
    "# price correlates with size related features and not so strongly with the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.box(figsize=(15, 8))\n",
    "plt.title(\"Distribution of Features\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_distribution.png\", dpi=150)\n",
    "plt.close()\n",
    "# most columnss have densly distributed values except price and sqft_lot which have more dispersion to the right, especially price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f056c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "df_scaled.plot.box(figsize=(15, 8))\n",
    "plt.title(\"Distribution of Features After Scaling\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_distribution_scaled.png\", dpi=150)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bfcb1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def print_report(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print(\n",
    "        f\"R² on train is {r2_score(y_train, y_pred_train):.4f}\",\n",
    "        f\"MSE: {mean_squared_error(y_train, y_pred_train):.4f}\",\n",
    "    )\n",
    "    print(\n",
    "        f\"R² on test is  {r2_score(y_test, y_pred_test):.4f}\",\n",
    "        f\"MSE: {mean_squared_error(y_test, y_pred_test):.4f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a88c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"price\")\n",
    "X_scaled = df_scaled.drop(columns=\"price\")\n",
    "target = df[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    target,\n",
    "    shuffle=True,\n",
    "    random_state=42, # we will reuse the split for scaled inputs\n",
    ")\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    X_scaled,\n",
    "    target,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "unscaled_features = {\n",
    "    \"X_train\": X_train,\n",
    "    \"X_test\":  X_test,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\":  y_test,\n",
    "}\n",
    "scaled_features = {\n",
    "    \"X_train\": X_train_scaled,\n",
    "    \"X_test\":  X_test_scaled,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\":  y_test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ba9f3",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b02587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline models\")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "print(\"LinearRegression\")\n",
    "print_report(lm, **unscaled_features)\n",
    "print_report(lm, **scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e46fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "print(\"KNN Regressor\")\n",
    "knn = KNeighborsRegressor().fit(X_train, y_train)\n",
    "print_report(knn, **unscaled_features)\n",
    "print_report(knn, **scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5eb94e",
   "metadata": {},
   "source": [
    "## Model Improvement\n",
    "### Handle multicollinearity by removing highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f283144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = X_train.corr().abs()\n",
    "print(\"Correlation matrix:\")\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3f074",
   "metadata": {},
   "source": [
    "Find pairs of features with high correlation (>0.8)\n",
    "Select upper triangle of correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ba583",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    ")\n",
    "\n",
    "# Find features with correlation greater than 0.8\n",
    "high_corr_pairs = []\n",
    "for i in range(len(upper_triangle.columns)):\n",
    "    for j in range(len(upper_triangle.columns)):\n",
    "        if i != j and upper_triangle.iloc[i, j] > 0.8:\n",
    "            high_corr_pairs.append((upper_triangle.index[i], upper_triangle.columns[j], upper_triangle.iloc[i, j]))\n",
    "\n",
    "print(\"Highly correlated feature pairs (correlation > 0.8):\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285f440",
   "metadata": {},
   "source": [
    "Identify features to drop based on high correlation (one from each pair, keeping the one with higher correlation to target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed97577",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = set()\n",
    "for feat1, feat2, corr_value in high_corr_pairs:\n",
    "    # Drop the feature with lower correlation to target (price)\n",
    "    corr_with_target_1 = abs(df[feat1].corr(df[\"price\"]))\n",
    "    corr_with_target_2 = abs(df[feat2].corr(df[\"price\"]))\n",
    "    if corr_with_target_1 < corr_with_target_2:\n",
    "        features_to_drop.add(feat1)\n",
    "    else:\n",
    "        features_to_drop.add(feat2)  # If equal, drop the second one\n",
    "\n",
    "print(f\"\\nFeatures to be dropped due to high correlation (>0.8): {list(features_to_drop)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38020ed",
   "metadata": {},
   "source": [
    "Create new datasets without highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ef74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_multicoll = X_train.drop(columns=list(features_to_drop))\n",
    "X_test_no_multicoll = X_test.drop(columns=list(features_to_drop))\n",
    "\n",
    "print(f\"Shape of X_train after removing multicollinear features: {X_train_no_multicoll.shape}\")\n",
    "print(f\"Shape of X_test after removing multicollinear features: {X_test_no_multicoll.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c2ff8",
   "metadata": {},
   "source": [
    "Train models with multicollinearity handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_no_multicoll = LinearRegression().fit(X_train_no_multicoll, y_train)\n",
    "print(\"\\nLinear Regression after handling multicollinearity\")\n",
    "print_report(lm_no_multicoll, X_train_no_multicoll, X_test_no_multicoll, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b132e7",
   "metadata": {},
   "source": [
    "Feature selection based on correlation with target variable\n",
    "Let's select features with higher correlation to price\n",
    "\n",
    "Calculate correlation with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_with_target = df.corr()[\"price\"].abs().sort_values(ascending=False)\n",
    "print(\"Features correlation with price:\")\n",
    "print(correlation_with_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bd76c",
   "metadata": {},
   "source": [
    "Select features with correlation above threshold and remove low-correlation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c72c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_threshold = 0.2\n",
    "selected_features = correlation_with_target[correlation_with_target > feature_threshold].index.tolist()\n",
    "selected_features = [feat for feat in selected_features if feat != \"price\"]  # Remove price from features\n",
    "print(f\"\\nFeatures selected with correlation > {feature_threshold}: {selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0c779",
   "metadata": {},
   "source": [
    "Apply both multicollinearity removal and low-correlation feature removal\n",
    "Start with all features, remove multicollinearity, then remove low-correlation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = set(X_train.columns)\n",
    "features_after_multicoll = all_features - features_to_drop\n",
    "features_after_multicoll_and_low_corr = [f for f in selected_features if f in features_after_multicoll]\n",
    "\n",
    "print(f\"\\nFeatures after removing both multicollinearity and low-correlation features: {features_after_multicoll_and_low_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8f37a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train models with features that survived both filters\n",
    "X_train_filtered = X_train[features_after_multicoll_and_low_corr]\n",
    "X_test_filtered = X_test[features_after_multicoll_and_low_corr]\n",
    "\n",
    "# Train Linear Regression with filtered features\n",
    "lm_filtered = LinearRegression().fit(X_train_filtered, y_train)\n",
    "print(\"\\nLinear Regression with Features Filtered (Multicollinearity + Low Correlation)\")\n",
    "print_report(lm_filtered, X_train_filtered, X_test_filtered, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec372c",
   "metadata": {},
   "source": [
    "Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column, factor=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Remove outliers from the target variable 'price'\n",
    "df_no_outliers = remove_outliers(df, 'price')\n",
    "print(f\"Shape before outlier removal: {df.shape}\")\n",
    "print(f\"Shape after outlier removal: {df_no_outliers.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c43b3",
   "metadata": {},
   "source": [
    "Re-split the data without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ecf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_outliers = df_no_outliers.drop(columns=\"price\")\n",
    "target_no_outliers = df_no_outliers[\"price\"]\n",
    "\n",
    "X_train_out, X_test_out, y_train_out, y_test_out = train_test_split(\n",
    "    X_no_outliers,\n",
    "    target_no_outliers,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Training set shape after outlier removal: {X_train_out.shape}\")\n",
    "print(f\"Test set shape after outlier removal: {X_test_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54671e3",
   "metadata": {},
   "source": [
    "Try ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557dc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import XGBoost if available\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgb_available = True\n",
    "except ImportError:\n",
    "    print(\"XGBoost not available, skipping XGBoost models\")\n",
    "    xgb_available = False\n",
    "    XGBRegressor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f02088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor on different feature sets\n",
    "# 1. Unfiltered, unscaled features\n",
    "rf_unfiltered = RandomForestRegressor(random_state=42)\n",
    "rf_unfiltered.fit(X_train, y_train)\n",
    "print(\"\\nRandom Forest Regressor (unfiltered, unscaled features)\")\n",
    "print_report(rf_unfiltered, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 2. Filtered, unscaled features\n",
    "rf_filtered = RandomForestRegressor(random_state=42)\n",
    "rf_filtered.fit(X_train_filtered, y_train)\n",
    "print(\"\\nRandom Forest Regressor (filtered, unscaled features)\")\n",
    "print_report(rf_filtered, X_train_filtered, X_test_filtered, y_train, y_test)\n",
    "\n",
    "# 3. Filtered, scaled features\n",
    "X_train_filtered_scaled = X_train_scaled[features_after_multicoll_and_low_corr]\n",
    "X_test_filtered_scaled = X_test_scaled[features_after_multicoll_and_low_corr]\n",
    "\n",
    "rf_filtered_scaled = RandomForestRegressor(random_state=42)\n",
    "rf_filtered_scaled.fit(X_train_filtered_scaled, y_train)\n",
    "print(\"\\nRandom Forest Regressor (filtered, scaled features)\")\n",
    "print_report(rf_filtered_scaled, X_train_filtered_scaled, X_test_filtered_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest on different feature sets\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# 1. Tuned Random Forest on unfiltered, unscaled features\n",
    "grid_search_rf_unfiltered = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid_rf,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_rf_unfiltered.fit(X_train, y_train)\n",
    "best_rf_unfiltered = grid_search_rf_unfiltered.best_estimator_\n",
    "\n",
    "print(\"\\nTuned Random Forest Regressor (unfiltered, unscaled features)\")\n",
    "print(f\"Best parameters: {grid_search_rf_unfiltered.best_params_}\")\n",
    "print_report(best_rf_unfiltered, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 2. Tuned Random Forest on filtered, unscaled features\n",
    "grid_search_rf_filtered = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid_rf,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_rf_filtered.fit(X_train_filtered, y_train)\n",
    "best_rf_filtered = grid_search_rf_filtered.best_estimator_\n",
    "\n",
    "print(\"\\nTuned Random Forest Regressor (filtered, unscaled features)\")\n",
    "print(f\"Best parameters: {grid_search_rf_filtered.best_params_}\")\n",
    "print_report(best_rf_filtered, X_train_filtered, X_test_filtered, y_train, y_test)\n",
    "\n",
    "# 3. Tuned Random Forest on filtered, scaled features\n",
    "grid_search_rf_filtered_scaled = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid_rf,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_rf_filtered_scaled.fit(X_train_filtered_scaled, y_train)\n",
    "best_rf_filtered_scaled = grid_search_rf_filtered_scaled.best_estimator_\n",
    "\n",
    "print(\"\\nTuned Random Forest Regressor (filtered, scaled features)\")\n",
    "print(f\"Best parameters: {grid_search_rf_filtered_scaled.best_params_}\")\n",
    "print_report(best_rf_filtered_scaled, X_train_filtered_scaled, X_test_filtered_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd66089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor on different feature sets\n",
    "# 1. Unfiltered, unscaled features\n",
    "gb_unfiltered = GradientBoostingRegressor(random_state=42)\n",
    "gb_unfiltered.fit(X_train, y_train)\n",
    "print(\"\\nGradient Boosting Regressor (unfiltered, unscaled features)\")\n",
    "print_report(gb_unfiltered, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 2. Filtered, unscaled features\n",
    "gb_filtered = GradientBoostingRegressor(random_state=42)\n",
    "gb_filtered.fit(X_train_filtered, y_train)\n",
    "print(\"\\nGradient Boosting Regressor (filtered, unscaled features)\")\n",
    "print_report(gb_filtered, X_train_filtered, X_test_filtered, y_train, y_test)\n",
    "\n",
    "# 3. Filtered, scaled features\n",
    "gb_filtered_scaled = GradientBoostingRegressor(random_state=42)\n",
    "gb_filtered_scaled.fit(X_train_filtered_scaled, y_train)\n",
    "print(\"\\nGradient Boosting Regressor (filtered, scaled features)\")\n",
    "print_report(gb_filtered_scaled, X_train_filtered_scaled, X_test_filtered_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad13e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Gradient Boosting on different feature sets\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# 1. Tuned Gradient Boosting on unfiltered, unscaled features\n",
    "grid_search_gb_unfiltered = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid_gb,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_gb_unfiltered.fit(X_train, y_train)\n",
    "best_gb_unfiltered = grid_search_gb_unfiltered.best_estimator_\n",
    "\n",
    "print(\"\\nTuned Gradient Boosting Regressor (unfiltered, unscaled features)\")\n",
    "print(f\"Best parameters: {grid_search_gb_unfiltered.best_params_}\")\n",
    "print_report(best_gb_unfiltered, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 2. Tuned Gradient Boosting on filtered, unscaled features\n",
    "grid_search_gb_filtered = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid_gb,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_gb_filtered.fit(X_train_filtered, y_train)\n",
    "best_gb_filtered = grid_search_gb_filtered.best_estimator_\n",
    "\n",
    "print(\"\\nTuned Gradient Boosting Regressor (filtered, unscaled features)\")\n",
    "print(f\"Best parameters: {grid_search_gb_filtered.best_params_}\")\n",
    "print_report(best_gb_filtered, X_train_filtered, X_test_filtered, y_train, y_test)\n",
    "\n",
    "# 3. Tuned Gradient Boosting on filtered, scaled features\n",
    "grid_search_gb_filtered_scaled = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid_gb,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_gb_filtered_scaled.fit(X_train_filtered_scaled, y_train)\n",
    "best_gb_filtered_scaled = grid_search_gb_filtered_scaled.best_estimator_\n",
    "\n",
    "print(\"\\nTuned Gradient Boosting Regressor (filtered, scaled features)\")\n",
    "print(f\"Best parameters: {grid_search_gb_filtered_scaled.best_params_}\")\n",
    "print_report(best_gb_filtered_scaled, X_train_filtered_scaled, X_test_filtered_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a324165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regressor on different feature sets\n",
    "# 1. Unfiltered, scaled features (SVR requires scaled features)\n",
    "svr_unfiltered_scaled = SVR()\n",
    "svr_unfiltered_scaled.fit(X_train_scaled, y_train)\n",
    "print(\"\\nSupport Vector Regressor (unfiltered, scaled features)\")\n",
    "print_report(svr_unfiltered_scaled, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "# 2. Filtered, scaled features\n",
    "svr_filtered_scaled = SVR()\n",
    "svr_filtered_scaled.fit(X_train_filtered_scaled, y_train)\n",
    "print(\"\\nSupport Vector Regressor (filtered, scaled features)\")\n",
    "print_report(svr_filtered_scaled, X_train_filtered_scaled, X_test_filtered_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15701086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for SVR on different feature sets\n",
    "param_grid_svr = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'epsilon': [0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# 1. Tuned SVR on unfiltered, scaled features\n",
    "grid_search_svr_unfiltered = GridSearchCV(\n",
    "    SVR(),\n",
    "    param_grid_svr,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_svr_unfiltered.fit(X_train_scaled, y_train)\n",
    "best_svr_unfiltered = grid_search_svr_unfiltered.best_estimator_\n",
    "\n",
    "print(\"\\nTuned Support Vector Regressor (unfiltered, scaled features)\")\n",
    "print(f\"Best parameters: {grid_search_svr_unfiltered.best_params_}\")\n",
    "print_report(best_svr_unfiltered, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "# 2. Tuned SVR on filtered, scaled features\n",
    "grid_search_svr_filtered = GridSearchCV(\n",
    "    SVR(),\n",
    "    param_grid_svr,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_svr_filtered.fit(X_train_filtered_scaled, y_train)\n",
    "best_svr_filtered = grid_search_svr_filtered.best_estimator_\n",
    "\n",
    "print(\"\\nTuned Support Vector Regressor (filtered, scaled features)\")\n",
    "print(f\"Best parameters: {grid_search_svr_filtered.best_params_}\")\n",
    "print_report(best_svr_filtered, X_train_filtered_scaled, X_test_filtered_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Regressor on different feature sets\n",
    "# 1. Unfiltered, unscaled features\n",
    "ada_unfiltered = AdaBoostRegressor(random_state=42)\n",
    "ada_unfiltered.fit(X_train, y_train)\n",
    "print(\"\\nAdaBoost Regressor (unfiltered, unscaled features)\")\n",
    "print_report(ada_unfiltered, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 2. Filtered, unscaled features\n",
    "ada_filtered = AdaBoostRegressor(random_state=42)\n",
    "ada_filtered.fit(X_train_filtered, y_train)\n",
    "print(\"\\nAdaBoost Regressor (filtered, unscaled features)\")\n",
    "print_report(ada_filtered, X_train_filtered, X_test_filtered, y_train, y_test)\n",
    "\n",
    "# 3. Filtered, scaled features\n",
    "ada_filtered_scaled = AdaBoostRegressor(random_state=42)\n",
    "ada_filtered_scaled.fit(X_train_filtered_scaled, y_train)\n",
    "print(\"\\nAdaBoost Regressor (filtered, scaled features)\")\n",
    "print_report(ada_filtered_scaled, X_train_filtered_scaled, X_test_filtered_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48016113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for AdaBoost on different feature sets\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.5, 1.0],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# 1. Tuned AdaBoost on unfiltered, unscaled features\n",
    "grid_search_ada_unfiltered = GridSearchCV(\n",
    "    AdaBoostRegressor(),\n",
    "    param_grid_ada,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_ada_unfiltered.fit(X_train, y_train)\n",
    "best_ada_unfiltered = grid_search_ada_unfiltered.best_estimator_\n",
    "\n",
    "print(\"\\nTuned AdaBoost Regressor (unfiltered, unscaled features)\")\n",
    "print(f\"Best parameters: {grid_search_ada_unfiltered.best_params_}\")\n",
    "print_report(best_ada_unfiltered, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 2. Tuned AdaBoost on filtered, unscaled features\n",
    "grid_search_ada_filtered = GridSearchCV(\n",
    "    AdaBoostRegressor(),\n",
    "    param_grid_ada,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_ada_filtered.fit(X_train_filtered, y_train)\n",
    "best_ada_filtered = grid_search_ada_filtered.best_estimator_\n",
    "\n",
    "print(\"\\nTuned AdaBoost Regressor (filtered, unscaled features)\")\n",
    "print(f\"Best parameters: {grid_search_ada_filtered.best_params_}\")\n",
    "print_report(best_ada_filtered, X_train_filtered, X_test_filtered, y_train, y_test)\n",
    "\n",
    "# 3. Tuned AdaBoost on filtered, scaled features\n",
    "grid_search_ada_filtered_scaled = GridSearchCV(\n",
    "    AdaBoostRegressor(),\n",
    "    param_grid_ada,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_ada_filtered_scaled.fit(X_train_filtered_scaled, y_train)\n",
    "best_ada_filtered_scaled = grid_search_ada_filtered_scaled.best_estimator_\n",
    "\n",
    "print(\"\\nTuned AdaBoost Regressor (filtered, scaled features)\")\n",
    "print(f\"Best parameters: {grid_search_ada_filtered_scaled.best_params_}\")\n",
    "print_report(best_ada_filtered_scaled, X_train_filtered_scaled, X_test_filtered_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Regressor on different feature sets (if available)\n",
    "if xgb_available:\n",
    "    # 1. Unfiltered, unscaled features\n",
    "    xgb_unfiltered = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    xgb_unfiltered.fit(X_train, y_train)\n",
    "    print(\"\\nXGBoost Regressor (unfiltered, unscaled features)\")\n",
    "    print_report(xgb_unfiltered, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # 2. Filtered, unscaled features\n",
    "    xgb_filtered = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    xgb_filtered.fit(X_train_filtered, y_train)\n",
    "    print(\"\\nXGBoost Regressor (filtered, unscaled features)\")\n",
    "    print_report(xgb_filtered, X_train_filtered, X_test_filtered, y_train, y_test)\n",
    "    \n",
    "    # 3. Filtered, scaled features\n",
    "    xgb_filtered_scaled = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    xgb_filtered_scaled.fit(X_train_filtered_scaled, y_train)\n",
    "    print(\"\\nXGBoost Regressor (filtered, scaled features)\")\n",
    "    print_report(xgb_filtered_scaled, X_train_filtered_scaled, X_test_filtered_scaled, y_train, y_test)\n",
    "    \n",
    "    # Hyperparameter tuning for XGBoost on different feature sets\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    # 1. Tuned XGBoost on unfiltered, unscaled features\n",
    "    grid_search_xgb_unfiltered = GridSearchCV(\n",
    "        XGBRegressor(random_state=42),\n",
    "        param_grid_xgb,\n",
    "        cv=3,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search_xgb_unfiltered.fit(X_train, y_train)\n",
    "    best_xgb_unfiltered = grid_search_xgb_unfiltered.best_estimator_\n",
    "    \n",
    "    print(\"\\nTuned XGBoost Regressor (unfiltered, unscaled features)\")\n",
    "    print(f\"Best parameters: {grid_search_xgb_unfiltered.best_params_}\")\n",
    "    print_report(best_xgb_unfiltered, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # 2. Tuned XGBoost on filtered, unscaled features\n",
    "    grid_search_xgb_filtered = GridSearchCV(\n",
    "        XGBRegressor(random_state=42),\n",
    "        param_grid_xgb,\n",
    "        cv=3,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search_xgb_filtered.fit(X_train_filtered, y_train)\n",
    "    best_xgb_filtered = grid_search_xgb_filtered.best_estimator_\n",
    "    \n",
    "    print(\"\\nTuned XGBoost Regressor (filtered, unscaled features)\")\n",
    "    print(f\"Best parameters: {grid_search_xgb_filtered.best_params_}\")\n",
    "    print_report(best_xgb_filtered, X_train_filtered, X_test_filtered, y_train, y_test)\n",
    "    \n",
    "    # 3. Tuned XGBoost on filtered, scaled features\n",
    "    grid_search_xgb_filtered_scaled = GridSearchCV(\n",
    "        XGBRegressor(random_state=42),\n",
    "        param_grid_xgb,\n",
    "        cv=3,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search_xgb_filtered_scaled.fit(X_train_filtered_scaled, y_train)\n",
    "    best_xgb_filtered_scaled = grid_search_xgb_filtered_scaled.best_estimator_\n",
    "    \n",
    "    print(\"\\nTuned XGBoost Regressor (filtered, scaled features)\")\n",
    "    print(f\"Best parameters: {grid_search_xgb_filtered_scaled.best_params_}\")\n",
    "    print_report(best_xgb_filtered_scaled, X_train_filtered_scaled, X_test_filtered_scaled, y_train, y_test)\n",
    "else:\n",
    "    print(\"\\nXGBoost not available, skipping XGBoost models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd9477",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d3934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store models, features, and results together for easier lookup\n",
    "models_and_results = {\n",
    "    \"Linear Regression\": {\n",
    "        \"model\": lm,\n",
    "        \"features\": X.columns,\n",
    "        \"R2_test\": r2_score(y_test, lm.predict(X_test)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, lm.predict(X_test)),\n",
    "        \"X_test\": X_test\n",
    "    },\n",
    "    \"Linear Regression (No Multicoll)\": {\n",
    "        \"model\": lm_no_multicoll,\n",
    "        \"features\": X_train_no_multicoll.columns,\n",
    "        \"R2_test\": r2_score(y_test, lm_no_multicoll.predict(X_test_no_multicoll)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, lm_no_multicoll.predict(X_test_no_multicoll)),\n",
    "        \"X_test\": X_test_no_multicoll\n",
    "    },\n",
    "    \"Linear Regression (Filtered)\": {\n",
    "        \"model\": lm_filtered,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, lm_filtered.predict(X_test_filtered)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, lm_filtered.predict(X_test_filtered)),\n",
    "        \"X_test\": X_test_filtered\n",
    "    },\n",
    "    # Random Forest models with different feature sets\n",
    "    \"Random Forest (Unfiltered, Unscaled)\": {\n",
    "        \"model\": rf_unfiltered,\n",
    "        \"features\": X_train.columns,\n",
    "        \"R2_test\": r2_score(y_test, rf_unfiltered.predict(X_test)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, rf_unfiltered.predict(X_test)),\n",
    "        \"X_test\": X_test\n",
    "    },\n",
    "    \"Random Forest (Filtered, Unscaled)\": {\n",
    "        \"model\": rf_filtered,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, rf_filtered.predict(X_test_filtered)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, rf_filtered.predict(X_test_filtered)),\n",
    "        \"X_test\": X_test_filtered\n",
    "    },\n",
    "    \"Random Forest (Filtered, Scaled)\": {\n",
    "        \"model\": rf_filtered_scaled,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, rf_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, rf_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"X_test\": X_test_filtered_scaled\n",
    "    },\n",
    "    # Tuned Random Forest models with different feature sets\n",
    "    \"Tuned Random Forest (Unfiltered, Unscaled)\": {\n",
    "        \"model\": best_rf_unfiltered,\n",
    "        \"features\": X_train.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_rf_unfiltered.predict(X_test)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_rf_unfiltered.predict(X_test)),\n",
    "        \"X_test\": X_test\n",
    "    },\n",
    "    \"Tuned Random Forest (Filtered, Unscaled)\": {\n",
    "        \"model\": best_rf_filtered,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_rf_filtered.predict(X_test_filtered)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_rf_filtered.predict(X_test_filtered)),\n",
    "        \"X_test\": X_test_filtered\n",
    "    },\n",
    "    \"Tuned Random Forest (Filtered, Scaled)\": {\n",
    "        \"model\": best_rf_filtered_scaled,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_rf_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_rf_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"X_test\": X_test_filtered_scaled\n",
    "    },\n",
    "    # Gradient Boosting models with different feature sets\n",
    "    \"Gradient Boosting (Unfiltered, Unscaled)\": {\n",
    "        \"model\": gb_unfiltered,\n",
    "        \"features\": X_train.columns,\n",
    "        \"R2_test\": r2_score(y_test, gb_unfiltered.predict(X_test)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, gb_unfiltered.predict(X_test)),\n",
    "        \"X_test\": X_test\n",
    "    },\n",
    "    \"Gradient Boosting (Filtered, Unscaled)\": {\n",
    "        \"model\": gb_filtered,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, gb_filtered.predict(X_test_filtered)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, gb_filtered.predict(X_test_filtered)),\n",
    "        \"X_test\": X_test_filtered\n",
    "    },\n",
    "    \"Gradient Boosting (Filtered, Scaled)\": {\n",
    "        \"model\": gb_filtered_scaled,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, gb_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, gb_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"X_test\": X_test_filtered_scaled\n",
    "    },\n",
    "    # Tuned Gradient Boosting models with different feature sets\n",
    "    \"Tuned Gradient Boosting (Unfiltered, Unscaled)\": {\n",
    "        \"model\": best_gb_unfiltered,\n",
    "        \"features\": X_train.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_gb_unfiltered.predict(X_test)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_gb_unfiltered.predict(X_test)),\n",
    "        \"X_test\": X_test\n",
    "    },\n",
    "    \"Tuned Gradient Boosting (Filtered, Unscaled)\": {\n",
    "        \"model\": best_gb_filtered,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_gb_filtered.predict(X_test_filtered)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_gb_filtered.predict(X_test_filtered)),\n",
    "        \"X_test\": X_test_filtered\n",
    "    },\n",
    "    \"Tuned Gradient Boosting (Filtered, Scaled)\": {\n",
    "        \"model\": best_gb_filtered_scaled,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_gb_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_gb_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"X_test\": X_test_filtered_scaled\n",
    "    },\n",
    "    # Support Vector Regressor models with different feature sets\n",
    "    \"SVR (Unfiltered, Scaled)\": {\n",
    "        \"model\": svr_unfiltered_scaled,\n",
    "        \"features\": X_train_scaled.columns,\n",
    "        \"R2_test\": r2_score(y_test, svr_unfiltered_scaled.predict(X_test_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, svr_unfiltered_scaled.predict(X_test_scaled)),\n",
    "        \"X_test\": X_test_scaled\n",
    "    },\n",
    "    \"SVR (Filtered, Scaled)\": {\n",
    "        \"model\": svr_filtered_scaled,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, svr_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, svr_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"X_test\": X_test_filtered_scaled\n",
    "    },\n",
    "    # Tuned SVR models with different feature sets\n",
    "    \"Tuned SVR (Unfiltered, Scaled)\": {\n",
    "        \"model\": best_svr_unfiltered,\n",
    "        \"features\": X_train_scaled.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_svr_unfiltered.predict(X_test_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_svr_unfiltered.predict(X_test_scaled)),\n",
    "        \"X_test\": X_test_scaled\n",
    "    },\n",
    "    \"Tuned SVR (Filtered, Scaled)\": {\n",
    "        \"model\": best_svr_filtered,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_svr_filtered.predict(X_test_filtered_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_svr_filtered.predict(X_test_filtered_scaled)),\n",
    "        \"X_test\": X_test_filtered_scaled\n",
    "    },\n",
    "    # AdaBoost models with different feature sets\n",
    "    \"AdaBoost (Unfiltered, Unscaled)\": {\n",
    "        \"model\": ada_unfiltered,\n",
    "        \"features\": X_train.columns,\n",
    "        \"R2_test\": r2_score(y_test, ada_unfiltered.predict(X_test)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, ada_unfiltered.predict(X_test)),\n",
    "        \"X_test\": X_test\n",
    "    },\n",
    "    \"AdaBoost (Filtered, Unscaled)\": {\n",
    "        \"model\": ada_filtered,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, ada_filtered.predict(X_test_filtered)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, ada_filtered.predict(X_test_filtered)),\n",
    "        \"X_test\": X_test_filtered\n",
    "    },\n",
    "    \"AdaBoost (Filtered, Scaled)\": {\n",
    "        \"model\": ada_filtered_scaled,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, ada_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, ada_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"X_test\": X_test_filtered_scaled\n",
    "    },\n",
    "    # Tuned AdaBoost models with different feature sets\n",
    "    \"Tuned AdaBoost (Unfiltered, Unscaled)\": {\n",
    "        \"model\": best_ada_unfiltered,\n",
    "        \"features\": X_train.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_ada_unfiltered.predict(X_test)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_ada_unfiltered.predict(X_test)),\n",
    "        \"X_test\": X_test\n",
    "    },\n",
    "    \"Tuned AdaBoost (Filtered, Unscaled)\": {\n",
    "        \"model\": best_ada_filtered,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_ada_filtered.predict(X_test_filtered)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_ada_filtered.predict(X_test_filtered)),\n",
    "        \"X_test\": X_test_filtered\n",
    "    },\n",
    "    \"Tuned AdaBoost (Filtered, Scaled)\": {\n",
    "        \"model\": best_ada_filtered_scaled,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_ada_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_ada_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"X_test\": X_test_filtered_scaled\n",
    "    },\n",
    "    # XGBoost models with different feature sets (if available)\n",
    "    **({\"XGBoost (Unfiltered, Unscaled)\": {\n",
    "        \"model\": xgb_unfiltered,\n",
    "        \"features\": X_train.columns,\n",
    "        \"R2_test\": r2_score(y_test, xgb_unfiltered.predict(X_test)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, xgb_unfiltered.predict(X_test)),\n",
    "        \"X_test\": X_test\n",
    "    }} if 'xgb_unfiltered' in locals() and xgb_available else {}),\n",
    "    **({\"XGBoost (Filtered, Unscaled)\": {\n",
    "        \"model\": xgb_filtered,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, xgb_filtered.predict(X_test_filtered)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, xgb_filtered.predict(X_test_filtered)),\n",
    "        \"X_test\": X_test_filtered\n",
    "    }} if 'xgb_filtered' in locals() and xgb_available else {}),\n",
    "    **({\"XGBoost (Filtered, Scaled)\": {\n",
    "        \"model\": xgb_filtered_scaled,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, xgb_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, xgb_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"X_test\": X_test_filtered_scaled\n",
    "    }} if 'xgb_filtered_scaled' in locals() and xgb_available else {}),\n",
    "    **({\"Tuned XGBoost (Unfiltered, Unscaled)\": {\n",
    "        \"model\": best_xgb_unfiltered,\n",
    "        \"features\": X_train.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_xgb_unfiltered.predict(X_test)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_xgb_unfiltered.predict(X_test)),\n",
    "        \"X_test\": X_test\n",
    "    }} if 'best_xgb_unfiltered' in locals() and xgb_available else {}),\n",
    "    **({\"Tuned XGBoost (Filtered, Unscaled)\": {\n",
    "        \"model\": best_xgb_filtered,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_xgb_filtered.predict(X_test_filtered)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_xgb_filtered.predict(X_test_filtered)),\n",
    "        \"X_test\": X_test_filtered\n",
    "    }} if 'best_xgb_filtered' in locals() and xgb_available else {}),\n",
    "    **({\"Tuned XGBoost (Filtered, Scaled)\": {\n",
    "        \"model\": best_xgb_filtered_scaled,\n",
    "        \"features\": X_train_filtered.columns,\n",
    "        \"R2_test\": r2_score(y_test, best_xgb_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"MSE_test\": mean_squared_error(y_test, best_xgb_filtered_scaled.predict(X_test_filtered_scaled)),\n",
    "        \"X_test\": X_test_filtered_scaled\n",
    "    }} if 'best_xgb_filtered_scaled' in locals() and xgb_available else {})\n",
    "}\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "results_data = {}\n",
    "for name, data in models_and_results.items():\n",
    "    results_data[name] = {k: v for k, v in data.items() if k in ['R2_test', 'MSE_test']}\n",
    "\n",
    "results_df = pd.DataFrame(results_data).T\n",
    "results_df = results_df.sort_values(by='R2_test', ascending=False)\n",
    "print(\"Model Comparison (sorted by R2 on test set):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad772de0",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis (using the best performing model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b795ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model based on R2 score\n",
    "best_model_name = results_df.index[0]\n",
    "\n",
    "# Use models_and_results dictionary to get model and features directly\n",
    "if best_model_name.startswith(\"SVR\"):\n",
    "    # SVR doesn't have feature importance, so use the best performing tree-based model for importance analysis\n",
    "    tree_models = [name for name in models_and_results.keys() if \"Random Forest\" in name or \"Gradient Boosting\" in name]\n",
    "    if tree_models:\n",
    "        # Find the best tree model among available ones\n",
    "        best_tree_model_name = None\n",
    "        best_r2 = -float('inf')\n",
    "        for name in tree_models:\n",
    "            if name in results_df.index and results_df.loc[name, 'R2_test'] > best_r2:\n",
    "                best_tree_model_name = name\n",
    "                best_r2 = results_df.loc[name, 'R2_test']\n",
    "\n",
    "        if best_tree_model_name:\n",
    "            best_model_for_importance = models_and_results[best_tree_model_name]['model']\n",
    "            feature_names = models_and_results[best_tree_model_name]['features']\n",
    "            final_model_name = f\"{best_tree_model_name} (for feature importance)\"\n",
    "        else:\n",
    "            # If no tree models, skip importance analysis\n",
    "            best_model_for_importance = None\n",
    "            feature_names = []\n",
    "            final_model_name = best_model_name\n",
    "    else:\n",
    "        # If no tree models in the available models, skip importance analysis\n",
    "        best_model_for_importance = None\n",
    "        feature_names = []\n",
    "        final_model_name = best_model_name\n",
    "else:\n",
    "    # For non-SVR models, use the actual best model\n",
    "    best_model_for_importance = models_and_results[best_model_name]['model']\n",
    "    feature_names = models_and_results[best_model_name]['features']\n",
    "    final_model_name = best_model_name\n",
    "\n",
    "print(f\"Best performing model: {final_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae489a9e",
   "metadata": {},
   "source": [
    "Extract feature importance based on model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_for_importance is not None and hasattr(best_model_for_importance, 'feature_importances_'):\n",
    "    # For tree-based models\n",
    "    feature_importance = best_model_for_importance.feature_importances_\n",
    "\n",
    "    # Create a dataframe for visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"\\nFeature Importances from Best Model:\")\n",
    "    print(importance_df)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=importance_df.head(10), x='Importance', y='Feature')\n",
    "    plt.title(f'Top 10 Feature Importances - {final_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_importance.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "elif best_model_for_importance is not None and hasattr(best_model_for_importance, 'coef_'):\n",
    "    # For linear models, use coefficients\n",
    "    feature_importance = np.abs(best_model_for_importance.coef_)\n",
    "\n",
    "    # Create a dataframe for visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient_Magnitude': feature_importance\n",
    "    }).sort_values('Coefficient_Magnitude', ascending=False)\n",
    "\n",
    "    print(\"\\nFeature Importance (Absolute Coefficient Values) from Linear Model:\")\n",
    "    print(importance_df)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=importance_df.head(10), x='Coefficient_Magnitude', y='Feature')\n",
    "    plt.title(f'Top 10 Feature Importance (Linear Model) - {final_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_importance_linear.png\", dpi=150)\n",
    "    plt.close()\n",
    "else:\n",
    "    print(f\"Feature importance not available for {final_model_name}\")\n",
    "    importance_df = pd.DataFrame()  # This will be handled in the final analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d194d",
   "metadata": {},
   "source": [
    "## Final Analysis and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf189fd5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Key findings:\")\n",
    "print(f\"1. Best model: {best_model_name}\")\n",
    "print(f\"2. Best R2 score on test set: {results_df.iloc[0]['R2_test']:.4f}\")\n",
    "print(f\"3. Best MSE on test set: {results_df.iloc[0]['MSE_test']:.2f}\")\n",
    "\n",
    "# Top 5 features that influence house prices\n",
    "if 'importance_df' in locals() or 'importance_df' in globals():\n",
    "    top_features = importance_df.head(5)['Feature'].tolist()\n",
    "    print(f\"4. Top 5 features influencing house prices: {top_features}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
